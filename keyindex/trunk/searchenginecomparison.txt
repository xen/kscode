Сравнительная характеристика и выбор средств для реализации поисковой системы
=============================================================================

Постановка задачи
-----------------

Для нашей задачи требования к поисковой системе состоят в следующем:

    - Возможность использования из * Python *
    - Высокая производительность при большом числе документов
    - Наличие документации и поддержки
    - Открытый исходный код
    - возможность создания на ее основе поискового сервера, который бы
      обслуживал запросы на поиск и индексирование документов
    - поддержка юникода
    - поддержка русского языка
    - поддержка морфологического поиска

Среди прочих рассмотрим такие системы как TextIndex, Xapian, PyLucene
#TODO Наверное надо включить сюда и PostgreSQL

Данный документ составлялся, используя документацию на рассмотренные поисковые
системы 


Обзор поисковых систем
----------------------

1. Основной язык реализации.

    TextIndex:
    
        * Python *. Поскольку реализация на самом * Python *, то проблем с
        интеграцией нет.
        
    Xapian:
        
        Библиотека написана на * С++ *. Импорт в * Python * делается с помощью
        * SWIG *.
        
        Для упрощения взаимодействия с клиентом существует библиотека xapwrap
        http://divmod.org/projects/xapwrap.        
        
    PyLucene:
    
        Библиотека написана на * Java *, скомпилированная с помощью * GCJ * 
        версия * Apache Lucene *.
        
        PyLucene скомпилирована как разделяемая библиотека, загружаемая как 
        расширение * Python * в процесс * Python * с помощью инструкции 
        'import'.

        Схема сборки PyLucene::
          -----------              --------------          -------------------
          | Java    | --- gcj ---> | lucene.o   |->------<-| PyLucene_wrap.o |
          | Lucene  | --- gcjh     --------------     |    -------------------
          -----------      |                          v             ^
                           v                   -----------------    |
                        ---------------        | _PyLucene.pyd |   g++
                        | C++ headers | -->|   -----------------    |
                        ---------------    |                        |
                                           |--- SWIG ---> ---------------------
          --------------                   |              | PyLucene_wrap.cxx |
          | PyLucene.i | ----------------->|              ---------------------
          --------------                                  |    PyLucene.py    |
                                                          ---------------------
2. Понимает или нет юникод.

    TextIndex:
    
        Поддерживает Python unicode-строки как для индексирования, так и для 
        поиска
        
    Xapian:
        
        В релизной версии поддерживается кодировка * iso-8859-1 *. В разработке
        находится версия, поддерживающая строки в * UTF-8 *.
        В библиотеках импорта unicode-строки кодируются в строки в * UTF-8 *.
        
    PyLucene:
    
        Поддерживает Python unicode-строки как для индексирования, так и для 
        поиска

3. Понимает или нет русский язык

    TextIndex:
    
        По умолчанию существует поддержка английского языка.
        Возможна поддержка любого языка с предоставлением разделителей,
        стоп-слов (и, или, но и т.д.), нормализатора
        
    Xapian:
        
        В релизной версии поддерживаются языки, использующие iso-8859-1:
        
            - danish (da)
            - dutch (nl)
            - english (en) - Martin Porter's 2002 revision of his stemmer
            - finnish (fi)
            - french (fr)
            - german (de)
            - italian (it)
            - norwegian (no)
            - portuguese (pt)
            - spanish (es)
            - swedish (sv)
        
        В текущей ветке разработки русский язык поддерживается:
        
            - russian (ru) (В текущей ветке разработки)
        
    PyLucene:
    
        Введен базовый класс Analyzer, который занимается получением терминов
        индекса из текста. Потомки этого класса для каждого языка по-своему
        реализуют выделение терминов.
        
        Для каждого поля документа возможна настройка на свой Analyzer.
        
        Реализованы следующие анализаторы:
        
            - BrazilianAnalyzer, 
            - ChineseAnalyzer, 
            - CJKAnalyzer, 
            - CzechAnalyzer, 
            - DutchAnalyzer, 
            - FrenchAnalyzer, 
            - GermanAnalyzer, 
            - GreekAnalyzer, 
            - RussianAnalyzer, 
             -ThaiAnalyzer
             
        Для руссокого языка Analyzer реализован, RussianAnalyzer    
    

4. Использование стемминга (встроеннное есть/нет, какой алгоритм), 
использование других лингвистических алгоритмов.

    TextIndex:
    
        Встроенной поддержки стемминга нет, возможна самостоятельная реализация
        (необходимо реализовать интерфейс zope.index.interfaces.ILexicon)
        
    Xapian:
        
        Есть встроенная поддержка некоторых языков (см. п 3). Алгоритм
        неизвестен
        
    PyLucene:
    
        Есть встроенная поддержка некоторых языков (см. п 3). Алгоритм
        неизвестен    

5. Механизм ранжирования

    TextIndex:
    
        Применяется вероятностная модель 
        #TODO: непонятно, что это такое?
        
        Ранжирование осуществляется по по некому алгоритму Okapi BM25 rank
        по такой формуле:
        
        score(D, Q) = sum(for t in D&Q: TF(D, t) * IDF(Q, t))

        где:
        
            D         документ
        
            Q         запрос
        
            t         термин (слово, фраза, т.д.)
        
            D&Q       термины встречающиеся одновременно в D и Q
        
            TF(D, t)  мера важности термина t в документе D -- вид веса частости
                      термина

            IDF(Q, t) мера важности t в запросе и в документах -- вид обратного
                      веса документа 
                      
                      #TODO: непонятно, что это такое?
    Xapian:
        
        Применяется похожий на предыдущий алгоритм, вроде бы даже BM25
        
        Вес документа расчитывается таким образом:

              ___
              \        (K' + 1) f'     (K + 1) f
               \              t              t
        W(D) =  |    ------------   ----------- w(t)
               /      K'L' + f'         KL + f
              /___               t               t
           t -> D, t in Q
        
        где:
        
            ft         вес t в D, частость t в D 
            
            L         нормализованая длина документа D, длина документа, деленная 
                    на среднюю длину документов в индексе
                    
            K         некоторая константа
            
            f't     вес t в Q
            
            L'         нормализованная длина запроса Q 

            K'         некоторая константа 
            
            w(t)    вес термина
                                (r + h) (N - R - n + r + h)
                    w(t) = log -----------------------------     
                                  (R - r + h) (n - r + h)
                                  
                    где:

                        h =  0.5
        
        Запись K+1 излишня, но помогает с интерпретацией выражения.
        Если K = 0 коэффициент перед w(t) равен 1, ф-ция не оказывает влияния.
        Если K стремится к бесконечности, коэффициент стремится к ft/L, 
        и ф-ция получает наибольшее влияние. 
        Наилучшее значение K зависит от характеристик индекса, и нет общих
        правил для выбора значения. Небольшие значения K рекомендуются для
        'безопасного' использования.
        
        Любой документ D имеет значение W(D), но, если ни один термин запроса
        не найден в индексе W(D) будет равно 0. 
        На практике имеют значение только документы, у которых W(D) > 0,
        и эти документы проиндексированы хотя бы одним термином из запроса Q.
        Если мы отсортируем по весу полученные документы то получим следующие
        пары документов и весов:
        
                           0:   D0  W(D0)
                          1:   D1  W(D1)
                          2:   D2  W(D2)
                                ....
                          K:   DK  W(DK)
        
        где W(Dj) >= W(Di) если j > i.
        
        Следуя вероятностной модели, документы D0, D1, D2 ранжируются по
        убыванию вероятности релевантности. Таким образом D0 имеет наибольшую
        вероятность быть релевантным, затем D1 и так далее.
        
        Xapian создает набор результатов (M set) из набора записей терминов\
        запроса.
        
        Порог отсечения, K, выбирается когда создан M set. Кандидаты на
        включение в набор результатов - это все документы, индексированные хотя
        бы одним термином из Q, и их число обычно превычит выбранный K.
        (K обычно устанавливается 1000 или меньше.) Набор M set это лучшие K 
        документов, найденные в результате поиска.
        
    PyLucene:
    
        Алгоритм расчета базируется на неком Vector Space Model
        
        http://en.wikipedia.org/wiki/Vector_Space_Model
        
        Формула:
                                                                         2
            score(q,d) = coord(q,d) · queryNorm(q) · ∑(tf(t в d) · idf(t) · t.getBoost() · norm(t,d))
                                                    t в q     

        где:
           
            1. tf(t в d) частость термина в документе. По умолчанию вычисление
               tf(t в d) в классе DefaultSimilarity:
       
                  tf(t в d) = частость½

       
            2. idf(t) представляет обратную частость документа. Эта ф-ция
               находится в связи с docFreq (числом документов, в которых
               встречается t). Это означает, что более редкие термины дают 
               больший вклад в общий ранг. По умолчанию вычисление idf(t) в
               классе DefaultSimilarity:
               
                                        numDocs
                  idf(t) = 1 + log (–––––––––––)
                                        docFreq+1
               
            3. coord(q,d) - коэффициент основанный на том, сколько терминов
               запроса найдено в данном документе. Обычно, документ, который
               содержит больше чем один термин запроса, получает больший ранг
               чем другой документ, с меньшим числом терминов запроса.
               Это коэффициент, действующий на этапе самого поиска.
               
            4. queryNorm(q) - нормализующий коэффициент используемый для
               сравнения рангов между запросами. Этот коэффициент не влияет на
               ранжирование (так как все ранжируемые документы умножаются на 
               один коэффициент), но зато позволяет сравнивать ранги из других
               запросов (или даже разных индексов). Это коэффициент, действующий
               на этапе самого поиска.
               По умолчанию вычисление в классе DefaultSimilarity:
               
                                                                           1
               queryNorm(q) = queryNorm(sumOfSquaredWeights) = –––––––––––––––––––––
                                                                sumOfSquaredWeights½
             
               Сумма квадратных корней весов (терминов запроса) вычисляется 
               обектом Weight запроса. Например, для булевый запрос вычисляет
               это значение таким образом:
                                                 2                             2
               sumOfSquaredWeights = q.getBoost() · ∑( idf(t) · t.getBoost()) 
                                                    t in q     
               
            5. t.getBoost() - это повышающий коэффициент ранга для термина
               запроса, указанный в тексте запроса, или установленный
               приложением. 
               
            6. norm(t,d) инксапсулирует несколько повыщающих ранг и длину
               коэффициентов во время индексирования:
                  
                  - Повышение документа перед добавлением в индекс.
                  
                  - Повышение поля документа перед добавлением в документ.
                  
                  - lengthNorm(field) - вычисляется в момент добавления
                    документа в индекс в соответствии с числом computed when the document is added to
                    the index in accordance with the number of лексем этого поля
                    в документе, таким образо более короткие поля получают
                    большее повышение
                    
               Когда документ добавляется в индекс, все вышеперечисленные коэф-
               фициенты перемножаются. Если документ имеет несколько полей с
               одинаковым именем, повышающие коэффициенты полей перемножаются:
               
               norm(t,d) = doc.getBoost() · lengthNorm(field) ·    ∏ f.getBoost()
                                                         field f в d названный как t     
        
              Впрочем, результирующее нормирующее значение кодируется как
              однобайтное перед сохранением. При поиске, нормирующий байт
              читается из каталога индекса и декодируется обратно в значение с
              плавающей точкой.
              Это перекодирование, связанное с уменьшением размера индекса,
              конечно, вызывает потерю точности - не гарантируется что
              decode(encode(x)) = x. Например decode(encode(0.89)) = 0.75.
              Также обратите внимание, что во время поиска слишком поздно моди-
              фицировать нормирующую часть ранжирования, например используя
              разные классы Similarity для поиска. 
            
6. Язык запросов, если язык запросов свой - какие возможности он предоставляет 
(например, можно записывать логические выражения над полями, используя полную 
булеву алгебру или усеченную и т.п., какие условия можно проверять);

    TextIndex:
    
        При индексировании документу ставится в соответствие набор терминов.
        В запросе применяются операции:
        
        - Логические операторы 'AND', 'OR', 'NOT'
        
        - Последовательность слов без операторов интерпретируется как 
          последовательность с операторами AND, например ` foo bar `
          
        - Текст в кавычках интерпретируется как поиск по фразе, 
          например, ` "foo bar" `
          
        - Слова, объединенные дефисом, интерпретируются как поиск по фразе, 
          например, ` foo-bar `
        
        - Дефис в качестве префикса интерпретируется как оператор NOT, например
          ` foo -bar `
        
        - Допускаются различные комбинации, например ` foo -"foo bar" ` или 
          ` foo -foo-bar `
        
        - * и ? используются для обобщения (например поиск по префиксу),
          например. ` foo* `
        
    Xapian:
        
        При индексировании документу ставится в соответствие набор терминов.
        В запросе применяются операции:
        
        - Логические операторы: 'AND', 'OR', 'NOT', 'XOR'
        
        - Cкобки в выражениях
        
          Допускается изменение приоритета условия за счет заключения его в
          скобки.

        - + и -
        
          Группа терминов помеченная '+' найдет документы содержащие все термины,
          помеченная '-' - ни одного. Допускается использование при поиске по
          фразам

        - NEAR
        
          one NEAR two NEAR three найдет документы, содержащие эти слова на 
          расстояниии не более 10 слов. Допускается указание порога n количества
          слов используя NEAR/n, например: one NEAR/6 two.

        - ADJ
        
          ADJ работает сходно с NEAR но слова находятся только в том порядке,
          в котором они указаны в запросе.

        - Текст в кавычках интерпретируется как поиск по фразе
         
          например, ` "foo bar" `. Соединенные дефисом или другим символом слова
          интерпретируются как фразы
          
        - Поиск по вероятностному полю
        
          Если база данных была проиндексирована с префиксами перед
          вероятностными терминами из определенных полей, попускается задать
          словарь префиксов таким образом, что пользователь сможет искать по
          полям. Например author:dickens title:shop должно найти документы с
          автором dickens и с shop в заголовке. 
          Также допускается использование префикса для фразы в кавычках или
          для выражения в скобках

        - Поиск по точному слову
        
          Если база данных была проиндексирована с словами в верхнем регистре,
          создавая термины с префиксами справа из слов целиком, без выделения
          корня, тогда поиск по слову в верхнем регистре найдет слово без
          применения морфологического поиска.
        
        - Поиск по корню слова
        
          Термин, заканчивающийся на . считается уже морфологически обработанным,
          то есть из него уже выделен корень
        
        - Обобщение
        
          Парсер запроса поддерживает использоваение '*' обощения, которое оз-
          начает любую последовательность символов, таким образом, wildc* 
          совпадет с wildcard, wildcarded, wildcards, wildcat, wildcats, т. д. 
        
    PyLucene:
    
        - Текст в кавычках интерпретируется как поиск по фразе, 
          
          например, ` "foo bar" `.

        - Поля

            Lucene поддерживает данные, находящиеся в полях. В условии поиска
            вы можете указать поле, или использовать поле по умолчанию. 
            Поиск по полю осуществляется указывая имя поля и термин запроса
            через ":". 
            
            Например:
             
             title:"The Right Way" AND text:go
            
            или:

            title:"Do it right" AND right
            
            Так как text - это поле по умолчанию, индикатор поля не нужен.
            Поле учитывает только для того термина, впереди которого оно указано
            Таким образом запрос:
            
            title:Do it right
            
            Найдет только "Do" в поле title. Он найдет "it" и "right" в поле
            по умолчанию (в нашем случае - text).
        
        - Модификаторы терминов
        
            Lucene поддерживает модификацию терминов запроса для обеспечния
            широкого диапазона параметров поиска.

        - Обобщенный поиск

            Lucene поддерживает обобщение для одного и нескольких символов.
            "?" - один символ
            "*" - любая последовательность символов
            
            Например: te?t, test*, te*t
            
            Примечание: Запрещено использование * или ? символа в начале
            термина запроса.

        - Нечеткий поиск

            Lucene поддерживает нечеткий поиск, основанный на Levenshtein
            Distance, или Edit Distance алгоритмах. 
            
            Для нечеткого поиска используется тильда, "~", в конце термина.
            Например для поиска по термину похожему на "roam" используйте
            нечеткий поиск:
            
            roam~
            
            Этот поиск найдет такие слова как foam и roams.
            
            Начиная с Lucene 1.9 дополнительный параметр регулирует похожесть.
            Значение в пределах от 0 до 1, со значением ближе к 1 будут найдены
            только термины с высокой похожестью и наоборот.
            Например:
            
            roam~0.8
            
            По умолчанию параметр равен 0.5.

        - Поиск по близости слов

            Lucene поддерживает поиск слов, находящихся на определенном расстоя-
            нии друг от друга.
            Для этого используется тильда, "~", в конце фразы, заключенной в 
            кавычки. Например, поиск по "apache" и "jakarta" на расстоянии до 10
            слов друг от друга:
            
            "jakarta apache"~10

        - Поиск по диапазону

            Запросы по диапазону позволяют находить документы со значениями поля
            (полей) лежащими в диапазоне между верхней и нижней границами
            запроса. Запрос может включать или исключать границы диапазона.
            Сортировка осуществляется лексикографически.
            
            Например:
            
            mod_date:[20020101 TO 20030101]
            
            Найдет документы со значением полей mod_date, между    20020101 и
            20030101, включительно. 
            
            title:{Aida TO Carmen}
            
            Найдет документы со значением полей title, между Aida и Carmen,
            исключительно. 
            
        - Усиление термина

            Lucene определяет уровни релевантности найденных документов,
            основанный на найденных терминах. Для усиления термина используйте
            символ "^", символ со степенью усиления (число) в конце термина, по
            которому осуществляется поиск.
            Например:
            
            jakarta apache
            
            вы хотите усилить термин "jakarta" для того чтобы он был более реле-
            вантным по отнощению к apache.
            Тогда:
            
            jakarta^4 apache
            
            Допускается использование усиления для поиска по фразам:            
            "jakarta apache"^4 "Apache Lucene"
            
            По умолчанию коэффициент усиления равен 1. Он должен быть
            положительным.

        - Логические операторы: 'AND', 'OR', 'NOT', 'XOR'
        
        - Cкобки в выражениях
        
          Допускается изменение приоритета условия за счет заключения его в
          скобки.

        - + и -
        
          Группа терминов помеченная '+' найдет документы содержащие все термины,
          помеченная '-' - ни одного. Допускается использование при поиске по
          фразам
        
        - Группировка по полям

            Lucene допускает использование скобок для группировки нескольких
            выражений для одного поля
            
            Например:
            
            title:(+return +"pink panther")

        - Специальные символы:

            Lucene поддерживает использование специальных символов, которые
            являются частью синтаксиса запроса. На данный момент список
            специальных символов такой:
            
            + - && || ! ( ) { } [ ] ^ " ~ * ? : \
            
            Для использования этих символов используйте \ перед символом,
            например:
            для поиска по (1+1):2 используйте запрос:
            
            \(1\+1\)\:2
            
7. Способ хранения данных (MySQL, Berkly DB, еще что-то);

    TextIndex:
    
        ZODB
        
    Xapian:
        
        Допускает хранение в различных базах данных:
        
        - quartz    
          Главный формат для Xapian, который может быть использован почти во
          всех случаях. Формат позволяет систематические(progressive #TODO: как правильно перевести?)
          изменения, single-writer multiple-reader доступ к базе данных, 
          и высокую эффективность доступа к данным.
        
        - flint    
          Flint это база данных "следующего поколения" для Xapian. 
          Имеет более высокую производительность и компактность с улучшенными
          механизмами блокировок и статистики.
          Но, поскольку Flint находится в активной разработке, формат часто
          меняется
          
        - da_flimsy    
          Это прогрессивный, унаследованный формат, поддерживающий базу
          данных в неизменяемой форме (то есть, база данных не может быть
          изменена, она собирается из существующей базы данных
          #TODO: непонятно, что имеется ввиду).
          Поддерживается доступ только-чтение 
          
        - da_heavy    
          Вариант da_flimsy, позволяющий доступ к варианту для больших
          документов. 
          
        - db_flimsy    
          Это другой прогрессивный, унаследованный формат, поддерживающий базу
          данных в изменяемой форме (то есть, база данных может быть
          изменена, пока к ней выполняются запросы).
          Поддерживается доступ только-чтение 
          
        - db_heavy
          Вариант da_flimsy, позволяющий доступ к варианту для больших
          документов. 

        - inmemory
          Этот тип базы данных держит данные полностью в памяти. Изначально написан 
          в целях тестирования, но может быть полезен для построения небольших
          временных баз данных
          
    PyLucene:
    
        Допускает хранение в 2-х вариантах:

        - FSDirectory, собственный формат. Данные разбиваются на сегменты, которые хранятся
          в отдельных файлах
        
          Ограничения:

              Число терминов и документов ограничено 32 битам, или примерно
              4 миллиарда. 
              
        - Berkley DB. 
          Популярный формат, отличающийся высокой надежностью и быстродействием
        
            
        
8. Интерфейс с основным приложением - библиотека, демон, что-то еще.

    TextIndex:
    
        Библиотека(пакет) на Python
        
    Xapian:
        
        Библиотека(пакет) на Python
        
    PyLucene:
    
        Библиотека(пакет) на Python
    

9. Протокол взаимодействия с приложением (xml-rpc, rpc, corba, http, webdav, 
fcgi, что-то еще);

    TextIndex:
    
        непосредственно вызовы атрибутов    
        
    Xapian:
        
        непосредственно вызовы атрибутов    
        
    PyLucene:

        непосредственно вызовы атрибутов    

10. Возможность одновременной обработки запросов, как реализовано 
(мультитредовый, мультипроцессорный, создание треда под запрос, создание 
процесса под запрос);

    TextIndex:

        Ограничено возможностями ZODB #TODO: вписать возможности ZODB
        
    Xapian:
        
        Изменение индекса возможно только в одном потоке, чтение потокобезопасно
        
    PyLucene:
    
        Изменение индекса возможно только в одном потоке, чтение потокобезопасно
        
    Для Xapian и PyLucene в случае применения для серверов приложений необходимо
    позаботиться о механизме блокировок на запись для индекса, так как возможны 
    одновременные запросы на изменение индекса
    
Выбор средств для реализации поисковой системы
----------------------------------------------

Рассмотрев перечисленные поисковые системы, легко определить, что при минимальных
затратах на написание кода, более всего для нашей задачи подходит библиотека
* PyLucene *.

Для реализации нашей системы нам понадобится, кроме самой библиотеки, построить
на ее основе производительный поисковый сервер, который бы смог обслуживать
запросы большого числа клиентов.

Для * PyLucene * существует такой сервер, * NXLucene *

Краткое описание NXLucene
-------------------------
 
http://www.cps-project.org/sections/projects/nxlucene

NXLucened - это автономный многопоточый удаленный сервер, управляющий хранилищем
индексов Lucene. Он базируется на PyLucene-импортами для Python и использует
Twisted и ZopeInterface для своей реализации.

На данный момент, он поддерживает протокол XML-RPC, но он может быть
легко расширен для других протоколов, таких как SOAP, благодаря его модульности.

На данный момент поддерживается хранение индексов Lucene только в "родном", 
FSDirectory-формате.

Результаты поиска выдаются в RSS-потоке.

NXLucened использует XML для описание запроса на индексирование и поиск.
Поиск в формате Lucene также поддерживается.

Существует также и клиентское решение для NXLucene под Zope 3, nuxeo.lucene

Краткое описание nuxeo.lucene
-----------------------------

http://svn.nuxeo.org/pub/Zope3/nuxeo.lucene

Этот пакет предоставляет реализацию Lucene-каталога для Zope3.

Он предполагает использование сервера nxlucene, и является, фактически, прокси,
предоставляющим конфигурационный, поисковый, индексирующий интерфейсы nxlucene
в Zope3. Непосредственно данных он не хранит. Все запросы перенаправляются на 
сервер nxlucene.

Выводы
------

Таким образом, мы определились со средствами для реализации поисковой системы
и выбрали такой набор:

    - PyLucene - поисковая библиотека
    
    - NXLucene - поисковый удаленный сервер
    
    - nuxeo.lucene - Zope3 интеграция клиента NXLucene-сервера, как каталог
                     Здесь нам необходимо будет переработать поведение продукта,
                     таким образом, чтобы его можно было использовать как индекс
                     в уже реализованном стандартном каталоге Zope3
                    
    #TODO: Сделать выделение всех названий, важных терминов
